{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1003\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1003\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1003\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemibrain\n",
      " +-- AL(L)*\n",
      " +-- AL(R)*\n",
      " +-- AOT(R)\n",
      " +-- CX\n",
      " |   +-- AB(L)*\n",
      " |   +-- AB(R)*\n",
      " |   +-- EB*\n",
      " |   +-- FB*\n",
      " |   +-- NO*\n",
      " |   +-- PB*\n",
      " +-- GC\n",
      " +-- GF(R)\n",
      " +-- GNG*\n",
      " +-- INP\n",
      " |   +-- ATL(L)*\n",
      " |   +-- ATL(R)*\n",
      " |   +-- CRE(L)*\n",
      " |   +-- CRE(R)*\n",
      " |   +-- IB*\n",
      " |   +-- ICL(L)*\n",
      " |   +-- ICL(R)*\n",
      " |   +-- SCL(L)*\n",
      " |   +-- SCL(R)*\n",
      " +-- LH(R)*\n",
      " +-- LX(L)\n",
      " |   +-- BU(L)*\n",
      " |   +-- LAL(L)*\n",
      " +-- LX(R)\n",
      " |   +-- BU(R)*\n",
      " |   +-- LAL(R)*\n",
      " +-- MB(+ACA)(R)\n",
      " |   +-- MB(R)\n",
      " |   |   +-- CA(R)*\n",
      " |   |   +-- PED(R)*\n",
      " |   |   +-- a'L(R)*\n",
      " |   |   +-- aL(R)*\n",
      " |   |   +-- b'L(R)*\n",
      " |   |   +-- bL(R)*\n",
      " |   |   +-- gL(R)*\n",
      " |   +-- dACA(R)\n",
      " |   +-- lACA(R)\n",
      " |   +-- vACA(R)\n",
      " +-- MB(L)\n",
      " |   +-- CA(L)*\n",
      " |   +-- a'L(L)*\n",
      " |   +-- aL(L)*\n",
      " |   +-- b'L(L)*\n",
      " |   +-- bL(L)*\n",
      " |   +-- gL(L)*\n",
      " +-- OL(R)\n",
      " |   +-- AME(R)*\n",
      " |   +-- LO(R)*\n",
      " |   +-- LOP(R)*\n",
      " |   +-- ME(R)*\n",
      " +-- PENP\n",
      " |   +-- CAN(R)*\n",
      " |   +-- FLA(R)*\n",
      " |   +-- PRW*\n",
      " |   +-- SAD*\n",
      " +-- POC\n",
      " +-- SNP(L)\n",
      " |   +-- SIP(L)*\n",
      " |   +-- SMP(L)*\n",
      " +-- SNP(R)\n",
      " |   +-- SIP(R)*\n",
      " |   +-- SLP(R)*\n",
      " |   +-- SMP(R)*\n",
      " +-- VLNP(R)\n",
      " |   +-- AOTU(R)*\n",
      " |   +-- AVLP(R)*\n",
      " |   +-- PLP(R)*\n",
      " |   +-- PVLP(R)*\n",
      " |   +-- WED(R)*\n",
      " +-- VMNP\n",
      " |   +-- EPA(L)*\n",
      " |   +-- EPA(R)*\n",
      " |   +-- GOR(L)*\n",
      " |   +-- GOR(R)*\n",
      " |   +-- IPS(R)*\n",
      " |   +-- SPS(L)*\n",
      " |   +-- SPS(R)*\n",
      " |   +-- VES(L)*\n",
      " |   +-- VES(R)*\n",
      " +-- mALT(L)\n",
      " +-- mALT(R)\n"
     ]
    }
   ],
   "source": [
    "# The plotting examples below require holoviews, hvplot, and bokeh:\n",
    "# conda install -c conda-forge bokeh holoviews hvplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bokeh\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "import bokeh.palettes\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()\n",
    "\n",
    "#import neuprint\n",
    "from neuprint import Client\n",
    "#register neuprint account, and get personal token\n",
    "c = Client('neuprint.janelia.org', dataset='hemibrain:v1.2.1', token='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6Inlhc2hzbWVodGE5NUBnbWFpbC5jb20iLCJsZXZlbCI6Im5vYXV0aCIsImltYWdlLXVybCI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hLS9BT2gxNEdqdnJKbW9vR1VqZHFDTzZCT3pVSGRMM2dZX1RDd2h4QnN6RDlDclhoST1zOTYtYz9zej01MD9zej01MCIsImV4cCI6MTgzNzkwMTU0M30.aJ_od2MDWIbqAh8c8Orx9TjbvCvsiH-v23TxC3dMg6M')\n",
    "c.fetch_version()\n",
    "from neuprint import NeuronCriteria as NC\n",
    "from neuprint import fetch_neurons\n",
    "from neuprint import fetch_adjacencies\n",
    "from neuprint import merge_neuron_properties\n",
    "\n",
    "from neuprint import fetch_roi_hierarchy\n",
    "## Show the ROI hierarchy, with primary ROIs marked with '*'\n",
    "print(fetch_roi_hierarchy(False, mark_primary=True, format='text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Select a specific body\n",
    "criteria = 387023620\n",
    "criteria = NC(bodyId=387023620)\n",
    "\n",
    "# Example: Select several bodies\n",
    "criteria = [387023620, 387364605, 416642425]\n",
    "criteria = NC(bodyId=[387023620, 387364605, 416642425])\n",
    "\n",
    "# Example: Select bodies by exact type\n",
    "criteria = 'PEN_b(PEN2)'\n",
    "criteria = NC(type='PENPEN_b(PEN2)')\n",
    "\n",
    "# Example: Select bodies by exact instance\n",
    "criteria = 'PEN(PB06)_b_L4'\n",
    "criteria = NC(type='PEN(PB06)_b_L4')\n",
    "\n",
    "# Example: Select bodies by type name pattern\n",
    "criteria = NC(type='PEN.*', regex=True)\n",
    "\n",
    "# Example: Select bodies by region (input or output)\n",
    "criteria = NC(rois=['PB', 'EB'])\n",
    "\n",
    "# Example: Select traced neurons which intersect the PB ROI with at least 100 inputs (PSDs).\n",
    "criteria = NC(inputRois=['PB'], min_roi_inputs=100, status='Traced', cropped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fetch_skeleton' from 'neuprint' (/groups/funke/home/mehtay/anaconda3/envs/plastix/lib/python3.9/site-packages/neuprint/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, fetch_neurons, fetch_skeleton, NeuronCriteria \u001b[38;5;28;01mas\u001b[39;00m NC, fetch_synapses, SynapseCriteria \u001b[38;5;28;01mas\u001b[39;00m SC, skeleton_segments\n\u001b[1;32m      3\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1136399017\u001b[39m\n\u001b[1;32m      4\u001b[0m synapses \u001b[38;5;241m=\u001b[39m fetch_synapses(body)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'fetch_skeleton' from 'neuprint' (/groups/funke/home/mehtay/anaconda3/envs/plastix/lib/python3.9/site-packages/neuprint/__init__.py)"
     ]
    }
   ],
   "source": [
    "from neuprint import Client, fetch_neurons, fetch_skeleton, NeuronCriteria as NC, fetch_synapses, SynapseCriteria as SC, skeleton_segments\n",
    "\n",
    "body = 1136399017\n",
    "synapses = fetch_synapses(body)\n",
    "skeleton = fetch_skeleton(body, heal=True)\n",
    "\n",
    "# Attach synapses to the skeleton at their nearest skeleton node\n",
    "augmented_skeleton = attach_synapses_to_skeleton(skeleton, synapses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fetch MBONs and DANs on the right hemisphere\n",
    "#fetch_neurons\n",
    "#MBON\n",
    "criteria = NC(instance='MBON.*_R',regex=True)\n",
    "neuron_MBON, roi_counts_MBON = fetch_neurons(criteria)\n",
    "#PAM\n",
    "criteria = NC(instance='PAM.*_R',regex=True)\n",
    "neuron_PAM, roi_counts_PAM = fetch_neurons(criteria)\n",
    "#PPL1\n",
    "criteria = NC(instance='PPL1.*_R',regex=True)\n",
    "neuron_PPL1, roi_counts_PPL1 = fetch_neurons(criteria)\n",
    "# PPL107 and PPL108 are not dopamine neurons\n",
    "# find value column equals to PPL107 and PPL108\n",
    "indexNames = neuron_PPL1[(neuron_PPL1['type'] == 'PPL107')|(neuron_PPL1['type'] == 'PPL108')].index\n",
    "# Delete these row indexes from dataFrame\n",
    "neuron_PPL1.drop(indexNames , inplace=True)\n",
    "#PPL2\n",
    "criteria = NC(instance='PPL2.*_R',regex=True)\n",
    "neuron_PPL2, roi_counts_PPL2 = fetch_neurons(criteria)\n",
    "# only PPL201 and PPL202 are not calyx-innervating dopamine neurons\n",
    "# find value column equals to PPL201 and PP202\n",
    "indexNames = neuron_PPL2[(neuron_PPL2['type'] == 'PPL201')|(neuron_PPL2['type'] == 'PPL202')].index\n",
    "# Delete these row indexes from dataFrame\n",
    "neuron_PPL2 = neuron_PPL2.loc[indexNames]\n",
    "\n",
    "neuron_MBON[['bodyId', 'instance', 'type', 'pre', 'post', 'status', 'cropped', 'size']]\n",
    "neuron_PAM[['bodyId', 'instance', 'type', 'pre', 'post', 'status', 'cropped', 'size']]\n",
    "neuron_PPL1[['bodyId', 'instance', 'type', 'pre', 'post', 'status', 'cropped', 'size']]\n",
    "neuron_PPL2[['bodyId', 'instance', 'type', 'pre', 'post', 'status', 'cropped', 'size']]\n",
    "neuron_DAN = pd.concat((neuron_PAM,neuron_PPL1,neuron_PPL2),axis=0)\n",
    "\n",
    "print('MBONs:'+str(neuron_MBON.shape[0]))\n",
    "print('DANs:'+str(neuron_DAN.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the neurons\n",
    "\n",
    "from bokeh.io import export_png\n",
    "#from bokeh.io import export_svg\n",
    "\n",
    "neuron_DF = neuron_DAN\n",
    "\n",
    "# Download some skeletons as DataFrames and attach columns for bodyId and color\n",
    "skeletons = []\n",
    "for i , bodyId in enumerate(neuron_DF['bodyId'].unique()):\n",
    "    s = c.fetch_skeleton(bodyId, format='pandas')\n",
    "    color_i = i%10\n",
    "    s['bodyId'] = bodyId\n",
    "    s['color'] = bokeh.palettes.Category10[10][color_i]\n",
    "    #s['color'] = bokeh.palettes.Viridis256[i*4]\n",
    "    skeletons.append(s)\n",
    "\n",
    "# Combine into one big table for convenient processing\n",
    "skeletons = pd.concat(skeletons, ignore_index=True)\n",
    "skeletons.head()\n",
    "\n",
    "# Join parent/child nodes for plotting as line segments below.\n",
    "segments = skeletons.merge(skeletons, 'inner',\n",
    "                           left_on=['bodyId', 'rowId'],\n",
    "                           right_on=['bodyId', 'link'],\n",
    "                           suffixes=['_child', '_parent'])\n",
    "\n",
    "p = figure()\n",
    "p.y_range.flipped = True\n",
    "p.background_fill_color = 'black'\n",
    "p.axis.visible = False\n",
    "p.grid.visible = False\n",
    "p.output_backend = \"svg\"\n",
    "\n",
    "# Plot skeleton segments (in 2D)\n",
    "p.segment(x0='x_child', x1='x_parent',\n",
    "          y0='z_child', y1='z_parent',\n",
    "          color='color_child',\n",
    "          source=segments)\n",
    "\n",
    "# Also plot the synapses from the above example\n",
    "# p.scatter(points['x_post'], points['z_post'], color=points['color'])\n",
    "\n",
    "show(p)\n",
    "export_png(p, filename=\"neuronSegments_DAN.png\")\n",
    "#export_svg(p, filename=\"neuron segments.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save bodyIds for later use\n",
    "bodyId_MBON = neuron_MBON['bodyId']\n",
    "bodyId_MBON.to_csv(\"./bodyId_MBON.csv\", sep=',',line_terminator=',',index=False)\n",
    "\n",
    "bodyId_PPL1 = neuron_PPL1['bodyId']\n",
    "bodyId_PPL1.to_csv(\"./bodyId_PPL1.csv\", sep=',',line_terminator=',',index=False)\n",
    "\n",
    "bodyId_PPL2 = neuron_PPL2['bodyId']\n",
    "bodyId_PPL2.to_csv(\"./bodyId_PPL2.csv\", sep=',',line_terminator=',',index=False)\n",
    "\n",
    "bodyId_PAM = neuron_PAM['bodyId']\n",
    "bodyId_PAM.to_csv(\"./bodyId_PAM.csv\", sep=',',line_terminator=',',index=False)\n",
    "\n",
    "bodyId_DAN = neuron_DAN['bodyId']\n",
    "bodyId_DAN.to_csv(\"./bodyId_DAN.csv\", sep=',',line_terminator=',',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Fetch all downstream connections FROM a set of neurons\n",
    "neuron_df, conn_df = fetch_adjacencies([387023620, 387364605, 416642425], None)\n",
    "\n",
    "# Example: Fetch all upstream connections TO a set of neurons\n",
    "neuron_df, conn_df = fetch_adjacencies(None, [387023620, 387364605, 416642425])\n",
    "\n",
    "# Example: Fetch all direct connections between a set of upstream neurons and downstream neurons\n",
    "neuron_df, conn_df = fetch_adjacencies(NC(type='Delta.*', regex=True), NC(type='PEN.*', regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get connection matrix\n",
    "# connection_table_to_matrix \n",
    "from neuprint.utils import connection_table_to_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fetch_adjacencies\n",
    "#get all the neurons and connections one-step downstream (DS1) of MBON\n",
    "neuron_DAN_IN1, roi_conn_DAN_IN1 = fetch_adjacencies(None,neuron_DAN)\n",
    "conn_DAN_IN1 = merge_neuron_properties(neuron_DAN_IN1, roi_conn_DAN_IN1, ['type', 'instance'])\n",
    "conn_DAN_IN1.sort_values('weight', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_DAN_by_bodyId = connection_table_to_matrix(conn_DAN_IN1, ('bodyId_pre', 'bodyId_post'),sort_by='bodyId')\n",
    "matrix_DAN_by_celltype = connection_table_to_matrix(conn_DAN_IN1, ('type_pre', 'type_post'),sort_by='type')\n",
    "matrix_DAN_by_celltype = matrix_DAN_by_celltype.dropna(axis=0, how='any') #remove those without identified cell types\n",
    "\n",
    "print('inputome bodyIds: '+str(matrix_DAN_by_bodyId.shape[0]))\n",
    "print('inputome celltypes: '+str(matrix_DAN_by_celltype.shape[0]))\n",
    "print('inputome: '+str(conn_DAN_IN1['weight'].sum())+' connections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain total weights of bodyId to bodyId for filtering (instead of per-connection-per-ROI weights)\n",
    "conn_groups = roi_conn_DAN_IN1.groupby(['bodyId_pre', 'bodyId_post'], as_index=False)\n",
    "total_conn_df = conn_groups['weight'].sum()\n",
    "total_conn_df.head()\n",
    "\n",
    "#plot distribution of weight and indicate the threshold\n",
    "x = total_conn_df.weight;\n",
    "bins = np.arange(0,51,1)\n",
    "bins = np.append(bins,1000)\n",
    "# the histogram of the data\n",
    "h,e = np.histogram(x, bins=bins)\n",
    "plt.bar(np.arange(0,len(bins)-1,1)+0.5,h, width=1, edgecolor='k')\n",
    "X_tick = [0,10,20,30,40,50]\n",
    "plt.xticks(X_tick)\n",
    "#n, bins, patches = plt.hist(x, bins=B, facecolor='blue', alpha=0.5,range=(0, 50))\n",
    "#Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "thisTitle = 'DAN input: '+str(matrix_DAN_by_bodyId.shape[0]) + ' neurons,' + str(matrix_DAN_by_celltype.shape[0]) + ' types'\n",
    "plt.title(thisTitle)\n",
    "plt.ylabel('Distribution of '+str(total_conn_df['weight'].sum(axis=0))+ ' connections')\n",
    "plt.xlabel('Connections per bodyId-bodyId pair')\n",
    "fname = 'Distribution DAN input weight no thresholding'\n",
    "plt.savefig(fname,dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set a threshold to ignore weak connections\n",
    "# Following Li 2020 elife, connection threshold was set at 5 for DAN inputs, and 10 for MBON outputs  \n",
    "indexNames = total_conn_df[total_conn_df['weight'] > 4].index\n",
    "count = 0\n",
    "for i in indexNames:\n",
    "    thisFind = roi_conn_DAN_IN1[(roi_conn_DAN_IN1['bodyId_pre'] == total_conn_df.loc[i,'bodyId_pre']) & (roi_conn_DAN_IN1['bodyId_post'] == total_conn_df.loc[i,'bodyId_post'])].index\n",
    "    count = count + 1\n",
    "    if count == 1:\n",
    "        sel_i_over4 = thisFind\n",
    "    else:\n",
    "        sel_i_over4 = sel_i_over4.append(thisFind)\n",
    "    #print(count)\n",
    "    #print(thisFind)\n",
    "    #print(sel_i_over4)\n",
    "roi_conn_DAN_IN1_over4 = roi_conn_DAN_IN1.loc[sel_i_over4]\n",
    "roi_conn_DAN_IN1_over4.shape\n",
    "\n",
    "#result after threholding\n",
    "conn_DAN_IN1_over4 = merge_neuron_properties(neuron_DAN_IN1, roi_conn_DAN_IN1_over4, ['type', 'instance'])\n",
    "conn_DAN_IN1_over4.sort_values('weight', ascending=False)\n",
    "conn_DAN_IN1_over4.shape\n",
    "matrix_DAN_by_bodyId = connection_table_to_matrix(conn_DAN_IN1_over4, ('bodyId_pre', 'bodyId_post'),sort_by='bodyId')\n",
    "matrix_DAN_by_celltype = connection_table_to_matrix(conn_DAN_IN1_over4, ('type_pre', 'type_post'),sort_by='type')\n",
    "matrix_DAN_by_celltype = matrix_DAN_by_celltype.dropna() #remove those without identified cell types\n",
    "\n",
    "print('inputome bodyIds: '+str(matrix_DAN_by_bodyId.shape[0]))\n",
    "print('inputome celltypes: '+str(matrix_DAN_by_celltype.shape[0]))\n",
    "\n",
    "# Obtain total weights (instead of per-connection-per-ROI weights)\n",
    "conn_groups = roi_conn_DAN_IN1_over4.groupby(['bodyId_pre', 'bodyId_post'], as_index=False)\n",
    "total_conn_df = conn_groups['weight'].sum()\n",
    "total_conn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot distribution of weight after thresholding\n",
    "x = total_conn_df.weight;\n",
    "# the histogram of the data\n",
    "bins = np.arange(0,51,1)\n",
    "bins = np.append(bins,1000)\n",
    "# the histogram of the data\n",
    "h,e = np.histogram(x, bins=bins)\n",
    "plt.bar(np.arange(0,len(bins)-1,1)+0.5,h, width=1, edgecolor='k')\n",
    "X_tick = [0,10,20,30,40,50]\n",
    "plt.xticks(X_tick)\n",
    "\n",
    "#Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "thisTitle = 'DAN input: ' + str(matrix_DAN_by_bodyId.shape[0]) + ' neurons,' + str(matrix_DAN_by_celltype.shape[0]) + ' types (weight > 4)'\n",
    "plt.title(thisTitle)\n",
    "plt.ylabel('Distribution of '+str(total_conn_df['weight'].sum(axis=0))+ ' connections')\n",
    "plt.xlabel('Connections per bodyId-bodyId pair')\n",
    "fname = 'Distribution DAN input weight over 4'\n",
    "plt.savefig(fname,dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the distribution of total input to the DAN layer\n",
    "\n",
    "x = matrix_DAN_by_celltype.sum(axis=1);\n",
    "# the histogram of the data\n",
    "bwidth = 10;\n",
    "bins = np.arange(0,500+bwidth,bwidth)\n",
    "bins = np.append(bins,5000)\n",
    "# the histogram of the data\n",
    "h,e = np.histogram(x, bins=bins)\n",
    "plt.bar(np.arange(0,(len(bins)-1)*bwidth,bwidth)+bwidth/2,h, width=bwidth, edgecolor='k')\n",
    "X_tick = [0,100,200,300,400,500]\n",
    "plt.xticks(X_tick)\n",
    "\n",
    "#num_bins = 200\n",
    "#n, bins, patches = plt.hist(x, num_bins, facecolor='blue', alpha=0.5,range=(0, 500))\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "thisTitle = 'Input to DANs from ' + str(matrix_DAN_by_celltype.shape[0]) + ' identified types'\n",
    "plt.title(thisTitle)\n",
    "plt.ylabel('Distribution of '+str(int(matrix_DAN_by_celltype.sum().sum()))+ ' connections')\n",
    "plt.xlabel('Summed connections to all DANs')\n",
    "fname = 'Input to DAN layer by cell type'\n",
    "plt.savefig(fname,dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show cell types with significant input into the DANs\n",
    "w_threshold = 200\n",
    "row_sum = matrix_DAN_by_celltype.sum(axis=1);\n",
    "matrix_significant_DAN_by_celltype = matrix_DAN_by_celltype.loc[row_sum>w_threshold,:]\n",
    "matrix_significant_DAN_by_celltype.shape\n",
    "matrix_significant_DAN_by_celltype.hvplot.heatmap(height=1000, width=600).opts(xrotation=60,title='total input>'+str(w_threshold))\n",
    "#hvplot.save(hMap, 'DAN input by celltype over 100.png', fmt='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_significant_DAN_by_celltype_excluding_KCs.index[0] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show cell types with significant input into the DANs\n",
    "# sum of input > 200, filtering out KCs, DPM, APL\n",
    "\n",
    "# exclude KCs, APL, DPM\n",
    "matrix_DAN_by_celltype_excluding_KCs = matrix_DAN_by_celltype\n",
    "A = matrix_DAN_by_celltype_excluding_KCs.index\n",
    "#find all KC columns\n",
    "test_list=set(filter(None, A))\n",
    "subs = 'KC'\n",
    "res = list(filter(lambda x: subs in x, test_list))\n",
    "#append APL&DPM\n",
    "res.append('APL')\n",
    "res.append('DPM')\n",
    "for i in range(len(res)): \n",
    "    matrix_DAN_by_celltype_excluding_KCs = matrix_DAN_by_celltype_excluding_KCs.loc[A!=res[i],:]\n",
    "    A = matrix_DAN_by_celltype_excluding_KCs.index\n",
    "\n",
    "# sum of input > 200\n",
    "row_sum = matrix_DAN_by_celltype_excluding_KCs.sum(axis=1);\n",
    "matrix_significant_DAN_by_celltype_excluding_KCs = matrix_DAN_by_celltype_excluding_KCs.loc[row_sum>w_threshold,:]#exclude the KCg type\n",
    "matrix_significant_DAN_by_celltype_excluding_KCs.shape\n",
    "matrix_significant_DAN_by_celltype_excluding_KCs.hvplot.heatmap(height=1000, width=600).opts(xrotation=60,title='total input>'+str(w_threshold)+', excluding KCs/APL/DPM')\n",
    "#hvplot.save(hMap, 'DAN input by celltype over 100.png', fmt='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = matrix_significant_DAN_by_celltype_excluding_KCs \n",
    "matrix_df.shape\n",
    "type_DAN = list(matrix_df.columns) \n",
    "type_DAN_App = ['PAM01_a','PAM01_b','PAM02','PAM04_a','PAM04_b','PAM08_a','PAM08_b','PAM08_c','PAM08_d','PAM08_e','PAM10','PAM11']#PAM07(y4<y1y2), PAM08_a(y4), PAM08_b(y4) - PAM09(b1pedc) \n",
    "type_DAN_Ave = ['PPL101','PPL103','PPL105','PPL106','PAM12','PAM03'] \n",
    "type_DAN_Unknown = set(type_DAN)-set(type_DAN_App)-set(type_DAN_Ave) \n",
    "print('type_DAN_Unknown:') \n",
    "print(type_DAN_Unknown)\n",
    "#type_DAN_Unknown = ['PPL202', 'PAM15_b', 'PAM06_b', 'PAM09', 'PPL104', 'PAM05', 'PPL102', 'PAM13', 'PAM14', 'PAM07', 'PAM15_a', 'PPL201', 'PAM06_a'] \n",
    "\n",
    "#reorder DAN order\n",
    "type_DAN = list(type_DAN_App) + list(type_DAN_Unknown) + list(type_DAN_Ave) \n",
    "matrix_df = matrix_df.loc[:,type_DAN]\n",
    "\n",
    "\n",
    "#sort by differential projection to appetitive and aversive compartments \n",
    "A = matrix_df.loc[:,type_DAN_App] \n",
    "A_row_sum = A.sum(1) \n",
    "B = matrix_df.loc[:,type_DAN_Ave] \n",
    "B_row_sum = B.sum(1) \n",
    "res = A_row_sum-B_row_sum \n",
    "sort_I = np.argsort(res) \n",
    "matrix_df = matrix_df.iloc[sort_I,:]\n",
    "\n",
    "#change DAN name to be conventional \n",
    "matrix_df.rename(columns = {\n",
    "    'PPL101':'PPL101(y1pedc)', 'PPL102':'PPL102(y1)','PPL103':'PPL103(y2ap1)','PPL104':'PPL104(ap3)','PPL105':'PPL105(ap2a2)','PPL106':'PPL106(a3)',\n",
    "    'PAM01_a':'PAM01(y5)_a','PAM01_b':'PAM01(y5)_b','PAM02':'PAM02(bp2a)','PAM03':'PAM03(b2bp2a)','PAM04_a':'PAM04(b2)_a','PAM04_b':'PAM04(b2)_b',\n",
    "    'PAM05':'PAM05(bp2p)','PAM06_a':'PAM06(bp2m)_a','PAM06_b':'PAM06(bp2m)_b','PAM07':'PAM07(y4-y1y2)',\n",
    "    'PAM08_a':'PAM08(y4)_a','PAM08_b':'PAM08(y4)_b','PAM08_c':'PAM08(y4)_c','PAM08_d':'PAM08(y4)_d','PAM08_e':'PAM08(y4)_e',\n",
    "    'PAM09':'PAM09(b1ped)','PAM10':'PAM10(b1)',\n",
    "    'PAM11':'PAM11(a1)','PAM12':'PAM12(y3)','PAM13':'PAM13(bp1ap)','PAM14':'PAM14(bp1m)','PAM15_a':'PAM15(y5bp2a)_a','PAM15_b':'PAM15(y5bp2a)_b'\n",
    "}, inplace = True)\n",
    "#matrix_df.columns = matrix_df.columns.str.replace('PPL101', 'PPL101(y1)')\n",
    "matrix_significant_DAN_by_celltype_excluding_KCs = matrix_df; \n",
    "\n",
    "matrix_df.hvplot.heatmap(height=800, width=600).opts(xrotation=60,title='total input>'+str(w_threshold)+', excluding KCs/APL/DPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the matrix\n",
    "matrix_DAN_by_celltype.to_csv(\"./DANinput_by_celltype.csv\", sep=',',line_terminator='\\n',index=True)\n",
    "matrix_significant_DAN_by_celltype.to_csv(\"./DANinput_by_celltype_significant.csv\", sep=',',line_terminator='\\n',index=True)\n",
    "matrix_significant_DAN_by_celltype_excluding_KCs.to_csv(\"./DANinput_by_celltype_significant_excluding_KCs.csv\", sep=',',line_terminator='\\n',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the 2-hop inputome\n",
    "type_DAN_IN1 = matrix_df.index\n",
    "\n",
    "type_DF = type_DAN_IN1#all\n",
    "#type_DF = type_DAN_IN1[0:24]#aversive\n",
    "#type_DF = type_DAN_IN1[-11:-1]#appetitive\n",
    "\n",
    "K = len(type_DF)\n",
    "neuron_DF = []\n",
    "for i in range(K):\n",
    "    criteria = NC(type=type_DF[i],regex=True)\n",
    "    neuron_df, roi_counts_df = fetch_neurons(criteria)\n",
    "    if len(neuron_DF) == 0:\n",
    "        neuron_DF = neuron_df\n",
    "    else:\n",
    "        neuron_DF = pd.concat((neuron_DF,neuron_df),axis=0)\n",
    "\n",
    "K = len(neuron_DF)\n",
    "neuron_df, conn_df = fetch_adjacencies(None,neuron_DF)\n",
    "conn_DF = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "\n",
    "is_meaningful_conn = conn_DF.weight>3\n",
    "conn_DF_over3 = conn_DF[is_meaningful_conn]\n",
    "conn_DF_over3 = conn_DF_over3.dropna(subset=['type_pre', 'type_post'])\n",
    "\n",
    "matrix_DF = connection_table_to_matrix(conn_DF_over3, ('type_pre', 'type_post'),sort_by='type')\n",
    "matrix_DF = matrix_DF.fillna(0)\n",
    "matrix_DF = matrix_DF.loc[matrix_DF.index!=None,:]\n",
    "matrix_DF.shape\n",
    "\n",
    "row_sum = matrix_DF.sum(1)\n",
    "matrix_significant_DF = matrix_DF.loc[row_sum>500,:]\n",
    "matrix_DAN_2hop_by_celltype = matrix_DF\n",
    "matrix_significant_DAN_2hop_by_celltype = matrix_significant_DF\n",
    "\n",
    "cap_value = 1000\n",
    "matrix_significant_DF[matrix_significant_DF>1000] = 1000\n",
    "A=matrix_significant_DF.loc[:,type_DF]\n",
    "A.hvplot.heatmap(height=1200, width=800).opts(xrotation=60,title='2-hop inputome for all DANs (weight>500)')\n",
    "#A.hvplot.heatmap(height=1200, width=600).opts(xrotation=60,title='2-hop inputome for aversive DANs (weight>200)')\n",
    "#A.hvplot.heatmap(height=1200, width=600).opts(xrotation=60,title='2-hop inputome for appetitive DANs (weight>200)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the 2 hop matrix\n",
    "matrix_DAN_2hop_by_celltype.to_csv(\"./DANinput_2hop_by_celltype.csv\", sep=',',line_terminator='\\n',index=True)\n",
    "matrix_significant_DAN_2hop_by_celltype.to_csv(\"./DANinput_2hop_by_celltype_significant.csv\", sep=',',line_terminator='\\n',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_DF = list(list(matrix_significant_DF.columns)+list(matrix_significant_DF.index))\n",
    "type_DF = list(set(type_DF))\n",
    "#find all KC columns\n",
    "test_list = type_DF\n",
    "subs = ['KC','APL','FB','Delta','FQ','MBON','PAM','DPM']\n",
    "K = len(subs)\n",
    "for i in range(K):\n",
    "    res = list(filter(lambda x: subs[i] in x, test_list))\n",
    "    type_DF = set(type_DF)-set(res)\n",
    " \n",
    "type_DF = list(type_DF)\n",
    "#type_DF = list(type_DF) + list(type_DAN_Ave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(type_DF)\n",
    "neuron_DF = []\n",
    "for i in range(K):\n",
    "    criteria = NC(instance=[type_DF[i]+'.*_R'],regex=True)\n",
    "    neuron_df, roi_counts_df = fetch_neurons(criteria)\n",
    "    if len(neuron_DF)==0:\n",
    "        neuron_DF = neuron_df\n",
    "    else:\n",
    "        neuron_DF = pd.concat((neuron_DF,neuron_df),axis=0)\n",
    "\n",
    "bodyId_DF = neuron_DF['bodyId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyId_DAN_Ave_inputome = neuron_DF['bodyId']\n",
    "bodyId_DAN_Ave_inputome.to_csv(\"./bodyId_DAN_Ave_inputome.csv\", sep=',',line_terminator=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(matrix_significant_DF.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_DF.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a fitting to reorder the matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = matrix_significant_DAN_by_celltype_excluding_KCs\n",
    "\n",
    "# https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "#rescale the data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "#optimal number of clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,80)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and transform the previous data\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(data_transformed)\n",
    "sort_I = np.argsort(km.labels_)\n",
    "data = data.iloc[sort_I,:]\n",
    "matrix_significant_DAN = data\n",
    "matrix_significant_DAN.hvplot.heatmap(height=1600, width=600).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sum = matrix_MBON.sum(0)\n",
    "x = col_sum\n",
    "cap_val = 1000\n",
    "x.loc[x>cap_val] = 1000\n",
    "num_bins = 200\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, num_bins, facecolor='blue', alpha=0.5)\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bodyId_MBoutput = neuron_MBON_DS1['bodyId']\n",
    "criteria = NC(bodyId=bodyId_MBoutput,regex=True)\n",
    "neuron_MBoutput, roi_counts_MBoutput = fetch_neurons(criteria)\n",
    "neuron_MBouput = neuron_MBoutput.loc[neuron_MBoutput['cropped']==False,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_MBoutput_notCropped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pd.DataFrame(neuron_MBoutput_notCropped['type'])\n",
    "neuron_MBoutput_notCropped['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_MBoutput_notCropped = neuron_MBoutput.loc[neuron_MBoutput['cropped']==False,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = neuron_MBoutput_notCropped['post']\n",
    "cap_val = 10000\n",
    "x.loc[x>cap_val] = 10000\n",
    "num_bins = 500\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, num_bins, facecolor='blue', alpha=0.5)\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.median(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_significant_MBON.shape\n",
    "matrix_significant_MBON.hvplot.heatmap(height=600, width=1200).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = matrix_significant_MBON.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a fitting to reorder the matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "#rescale the data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "#optimal number of clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,80)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort and transform the previous data\n",
    "k = 40\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(data_transformed)\n",
    "sort_I = np.argsort(km.labels_)\n",
    "data = data.iloc[sort_I,:]\n",
    "matrix_significant_MBON = data.transpose()\n",
    "matrix_significant_MBON.hvplot.heatmap(height=600, width=1200).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_significant_MBON.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the cell types in the MB output pathway\n",
    "type_MBoutput = []\n",
    "type_MBON = []\n",
    "\n",
    "#pool together all the MBONs\n",
    "for i in range(len(matrix_significant_MBON.index)):\n",
    "    a = matrix_significant_MBON.index[i]\n",
    "    type_MBoutput.append(a)\n",
    "type_MBON = type_MBoutput\n",
    "len(type_MBON)\n",
    "outfile=open('type_MBON.txt', 'w'); # Open file for writing\n",
    "outfile.write(\",\".join(type_MBON)); # Write the list onto the file\n",
    "outfile.close(); # Flush and close the IO object\n",
    "\n",
    "#pool together the additional MBON downstream\n",
    "for i in range(len(matrix_significant_MBON.columns)):\n",
    "    a = matrix_significant_MBON.columns[i]\n",
    "    type_MBoutput.append(a)\n",
    "type_MBoutput = list(set(type_MBoutput))\n",
    "len(type_MBoutput)\n",
    "outfile=open('type_MBoutput.txt', 'w'); # Open file for writing\n",
    "outfile.write(\",\".join(type_MBoutput)); # Write the list onto the file\n",
    "outfile.close(); # Flush and close the IO object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(type_MBoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fetch MBoutput that have connection to LAL\n",
    "from neuprint import fetch_synapse_connections, SynapseCriteria as SC\n",
    "\n",
    "syn_DF = []\n",
    "type_df = type_MBoutput\n",
    "#type_df = type_MBON\n",
    "syn_criteria_df = SC(rois=['LAL(R)','LAL(L)'], primary_only=True)\n",
    "syn_criteria_df = None\n",
    "\n",
    "K = len(type_df)\n",
    "for i in range(K):    \n",
    "    neuron_criteria = NC(status='Traced', type=type_df[i], cropped=False, min_pre=3)\n",
    "    try:\n",
    "        syn_df = fetch_synapse_connections(neuron_criteria, None, syn_criteria_df)\n",
    "        print(str(i)+'<-> :found connection(s)')\n",
    "        if len(syn_DF)>0: \n",
    "                syn_DF = pd.concat([syn_DF,syn_df], axis=0, join='outer', ignore_index=True,sort=False)        \n",
    "        else:\n",
    "                syn_DF = syn_df\n",
    "    except:\n",
    "            print(str(i)+'<-> :no connection')\n",
    "    \n",
    "# retieve the types of pre-synaptic neurons\n",
    "pre_neurons, _ = fetch_neurons(syn_DF['bodyId_pre'].unique())\n",
    "syn_DF = merge_neuron_properties(pre_neurons, syn_DF, 'type')\n",
    "\n",
    "#syn_MBoutput_LAL = syn_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "plt1 = fig.add_subplot(2,1,1)\n",
    "plt1.hist(syn_DF['type_pre'].value_counts(),200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(type_MBoutput)\n",
    "S = []\n",
    "for i in range(K):\n",
    "    A = syn_DF.loc[syn_DF['type_pre']==type_MBoutput[i],:]\n",
    "    s = A['roi_pre'].value_counts()\n",
    "    s = pd.DataFrame(s)\n",
    "    s.columns = [type_MBoutput[i]]\n",
    "    if i==0:\n",
    "        S = s\n",
    "    else:\n",
    "        S = pd.concat([S,s], axis=1, join='outer', ignore_index=False,sort=False)        \n",
    "\n",
    "S = S.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = S.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a fitting to reorder the matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "#rescale the data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "#optimal number of clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,40)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and transform the previous data\n",
    "k = 40\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(data_transformed)\n",
    "sort_I = np.argsort(km.labels_)\n",
    "data = data.iloc[sort_I,:]\n",
    "S = data.transpose()\n",
    "S.hvplot.heatmap(height=600, width=1200).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale the data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "#optimal number of clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,40)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort and transform the previous data\n",
    "k = 25\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(data_transformed)\n",
    "sort_I = np.argsort(km.labels_)\n",
    "data = data.iloc[sort_I,:]\n",
    "S = data\n",
    "S.hvplot.heatmap(height=800, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.hvplot.heatmap(height=600, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_DF.loc[type_MBoutput[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuprint import fetch_roi_hierarchy\n",
    "## Show the ROI hierarchy, with primary ROIs marked with '*'\n",
    "print(fetch_roi_hierarchy(False, mark_primary=True, format='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "syn_DF['type_pre'].value_counts()\n",
    "#syn_DF['type_post'].value_counts()\n",
    "type_presyn_DF = syn_DF['type_pre'].unique()\n",
    "type_postsyn_DF = syn_DF['type_post'].unique()\n",
    "type_presyn_DF = list(set(type_presyn_DF)&set(type_MBoutput))\n",
    "type_postsyn_DF = list(set(type_postsyn_DF)&set(type_MBoutput))\n",
    "\n",
    "len(type_presyn_DF)\n",
    "\n",
    "# Retrieve the types of pre-synaptic neurons\n",
    "top10_counts = syn_DF['type_pre'].value_counts().head(10)\n",
    "top10_counts\n",
    "colormap = dict(zip(top10_counts.index, bokeh.palettes.Category10[10]))\n",
    "points = syn_DF.query('type_pre in @top10_counts.index').copy()\n",
    "points['color'] = points['type_pre'].map(colormap)\n",
    "\n",
    "#p = figure()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "plt1 = fig.add_subplot(2,1,1)\n",
    "plt1.hist(syn_DF['type_pre'].value_counts(),[0,100,200,300,400,500,600,700,800,900,1000])\n",
    "\n",
    "plt2 = fig.add_subplot(2,1,2)\n",
    "plt2.scatter(points['x_post'], points['z_post'], color=points['color'])\n",
    "plt2.axis('equal')\n",
    "plt2.set_title('top 10 innervation')\n",
    "\n",
    "plt\n",
    "\n",
    "\n",
    "#axs[0, 0].set_title('top10 innervation from MB output ')\n",
    "\n",
    "#axs[0,1].hist(syn_DF['type_pre'].value_counts())\n",
    "#p.y_range.flipped = True\n",
    "#show(p)\n",
    "\n",
    "\n",
    "#handlelist = [plt.plot([], marker=\"o\", ls=\"\", color=color)[0] for color in colormap]\n",
    "#plt.legend(handlelist,top10_counts.index,loc='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select those significant ones\n",
    "type_MBoutput_DF = syn_DF['type_pre'].value_counts()\n",
    "type_MBoutput_DF = type_MBoutput_DF.loc[type_MBoutput_DF>100]\n",
    "len(type_MBoutput_DF)\n",
    "type_MBoutput_DF\n",
    "\n",
    "#store the LAL data\n",
    "syn_MBoutput_LAL = syn_DF\n",
    "type_MBoutput_LAL_sig = type_MBoutput_DF\n",
    "syn_MBoutput_LAL_sig = syn_DF.query('type_pre in @type_MBoutput_LAL_sig.index').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some skeletons as DataFrames and attach columns for bodyId and color\n",
    "skeletons = []\n",
    "for i , bodyId in enumerate(syn_MBoutput_LAL_sig['bodyId_pre'].unique()):\n",
    "    s = c.fetch_skeleton(bodyId, format='pandas')\n",
    "    s['bodyId'] = bodyId\n",
    "    s['color'] = bokeh.palettes.Category10[10][1]\n",
    "    skeletons.append(s)\n",
    "\n",
    "# Combine into one big table for convenient processing\n",
    "skeletons = pd.concat(skeletons, ignore_index=True)\n",
    "skeletons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skeletons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join parent/child nodes for plotting as line segments below.\n",
    "segments = skeletons.merge(skeletons, 'inner',\n",
    "                           left_on=['bodyId', 'rowId'],\n",
    "                           right_on=['bodyId', 'link'],\n",
    "                           suffixes=['_child', '_parent'])\n",
    "\n",
    "p = figure()\n",
    "p.y_range.flipped = True\n",
    "\n",
    "# Plot skeleton segments (in 2D)\n",
    "p.segment(x0='x_child', x1='x_parent',\n",
    "          y0='z_child', y1='z_parent',\n",
    "          color='color_child',\n",
    "          source=segments)\n",
    "\n",
    "# Also plot the synapses from the above example\n",
    "p.scatter(points['x_post'], points['z_post'], color=points['color'])\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the whole MB output network\n",
    "# this pulling takes quite a bit of time\n",
    "neuron_MBoutput = []\n",
    "conn_MBoutput = []\n",
    "K = len(type_MBoutput)\n",
    "for i in range(K):\n",
    "    for ii in range(K):\n",
    "        try:\n",
    "            neuron_df, conn_df = fetch_adjacencies(NC(type=type_MBoutput[i], regex=True), NC(type=type_MBoutput[ii], regex=True),min_total_weight=3)\n",
    "            print(str(i)+'->'+str(ii)+':found connection(s)')\n",
    "            if len(neuron_MBoutput)>0: \n",
    "                neuron_MBoutput = pd.concat([neuron_MBoutput,neuron_df], axis=0, join='outer', ignore_index=True,sort=False)        \n",
    "                conn_MBoutput = pd.concat([conn_MBoutput,conn_df], axis=0, join='outer', ignore_index=True,sort=False)        \n",
    "            else:\n",
    "                neuron_MBoutput = neuron_df\n",
    "                conn_MBoutput = conn_df\n",
    "        except:\n",
    "            print(str(i)+'->'+str(ii)+':no connection')\n",
    "\n",
    "#there is some bugs in the program which bring in a lot of redundancy - need fixation later\n",
    "neuron_MBoutput = neuron_MBoutput.drop_duplicates(keep='first')\n",
    "conn_MBoutput = conn_MBoutput.drop_duplicates(keep='first')            \n",
    "            \n",
    "conn_MBoutput = merge_neuron_properties(neuron_MBoutput, conn_MBoutput, ['type', 'instance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pulling is so time-consuming, make a backup\n",
    "neuron_MBoutput_backup = neuron_MBoutput\n",
    "conn_MBoutput_backup = conn_MBoutput\n",
    "# and save them\n",
    "neuron_MBON.to_csv('neuron_MBON.csv')\n",
    "roi_counts_MBON.to_csv('roi_counts_MBON.csv')\n",
    "neuron_PAM.to_csv('neuron_PAM.csv')\n",
    "roi_counts_PAM.to_csv('roi_counts_PAM.csv')\n",
    "neuron_PPL1.to_csv('neuron_PPL1.csv')\n",
    "roi_counts_PPL1.to_csv('roi_counts_PPL1.csv')\n",
    "neuron_MBON_DS1.to_csv('neuron_MBON_DS1.csv')\n",
    "conn_MBON_DS1.to_csv('conn_MBON_DS1.csv')\n",
    "conn_MBON_DS1_over3.to_csv('conn_MBON_DS1_over3.csv')\n",
    "neuron_MBoutput.to_csv('neuron_MBoutput.csv')\n",
    "conn_MBoutput.to_csv('conn_MBoutput.csv')\n",
    "neuron_MBoutput_backup.to_csv('neuron_MBoutput_backup.csv')\n",
    "conn_MBoutput_backup.to_csv('conn_MBoutput_backup.csv')\n",
    "matrix_MBON.to_csv('matrix_MBON.csv')\n",
    "matrix_significant_MBON.to_csv('matrix_significant_MBON.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_MBoutput = connection_table_to_matrix(conn_MBoutput, 'type')\n",
    "matrix_MBoutput.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = matrix_MBoutput.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a fitting to reorder the matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "#rescale the data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "#optimal number of clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,80)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and transform the previous data\n",
    "k = 40\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(data_transformed)\n",
    "sort_I = np.argsort(km.labels_)\n",
    "data = data.iloc[sort_I,:]\n",
    "matrix_MBoutput = data.transpose()\n",
    "matrix_MBoutput.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = matrix_MBoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a fitting to reorder the matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "#rescale the data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(data)\n",
    "data_transformed = mms.transform(data)\n",
    "#optimal number of clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,80)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and transform the previous data\n",
    "k = 40\n",
    "km = KMeans(n_clusters=k)\n",
    "km = km.fit(data_transformed)\n",
    "sort_I = np.argsort(km.labels_)\n",
    "data = data.iloc[sort_I,:]\n",
    "matrix_MBoutput = data\n",
    "matrix_MBoutput.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_MBoutput.to_csv('matrix_MBoutput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove MBON in the type_post\n",
    "# exclude APL KCs\n",
    "A = matrix_MBoutput\n",
    "#find all KC columns\n",
    "test_list=matrix_MBoutput.columns\n",
    "#subs = 'KC'\n",
    "#res = list(filter(lambda x: subs in x, test_list))\n",
    "res = list(neuron_MBON.type)\n",
    "for i in range(len(res)): \n",
    "    A = A.loc[:,A.columns!=res[i]]\n",
    "#matrix_MBoutput = A\n",
    "matrix_MBoutput_noMBON = A\n",
    "matrix_MBoutput_noMBON.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove MBON in the type_post\n",
    "# exclude APL KCs\n",
    "A = matrix_MBoutput\n",
    "#find all KC columns\n",
    "test_list=matrix_MBoutput.columns\n",
    "#subs = 'KC'\n",
    "#res = list(filter(lambda x: subs in x, test_list))\n",
    "res = list(neuron_MBON.type.unique())\n",
    "A = A[res]\n",
    "#matrix_MBoutput = A\n",
    "matrix_MBoutput_onlyMBON = A\n",
    "matrix_MBoutput_onlyMBON.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = 'FB'\n",
    "res = list(filter(lambda x: subs in x, test_list))\n",
    "A = matrix_MBoutput[res]\n",
    "matrix_MBoutput_onlyFB = A\n",
    "A.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = 'PPL'\n",
    "res1 = list(filter(lambda x: subs in x, test_list))\n",
    "subs = 'PAM'\n",
    "res2 = list(filter(lambda x: subs in x, test_list))\n",
    "res = res1+res2\n",
    "A = matrix_MBoutput[res]\n",
    "matrix_MBoutput_onlyDAN = A\n",
    "A.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subs = 'AVM07'\n",
    "res1 = list(filter(lambda x: subs in x, test_list))\n",
    "subs = 'PDL05'\n",
    "res2 = list(filter(lambda x: subs in x, test_list))\n",
    "res = res1+res2\n",
    "A = matrix_MBoutput[res]\n",
    "matrix_MBoutput_AVM07_PDL05 = A\n",
    "A.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's left \n",
    "A = matrix_MBoutput\n",
    "res = list(neuron_MBON.type.unique())\n",
    "exclude_list = ('PPL1','PAM','FB','AVM07','PDL05')\n",
    "K = len(exclude_list)\n",
    "for i in range(K):\n",
    "    subs = exclude_list[i]\n",
    "    x = list(filter(lambda x: subs in x, test_list))\n",
    "    res = res+x\n",
    "\n",
    "for i in range(len(res)): \n",
    "    A = A.loc[:,A.columns!=res[i]]\n",
    "#matrix_MBoutput = A\n",
    "matrix_MBoutput_remaining = A\n",
    "matrix_MBoutput_remaining.hvplot.heatmap(height=1500, width=1500).opts(xrotation=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct some errors\n",
    "\"\"\"\n",
    "A = []\n",
    "for i in range(len(neuron_MBoutput)):\n",
    "    if i==0:\n",
    "        A = neuron_MBoutput[i]\n",
    "    else:\n",
    "        A = pd.concat([A,neuron_MBoutput[i]], axis=0, join='outer', ignore_index=True,sort=False)        \n",
    "neuron_MBoutput = A\n",
    "\n",
    "B = []\n",
    "for i in range(len(conn_MBoutput)):\n",
    "    if i==0:\n",
    "        B = conn_MBoutput[i]\n",
    "    else:\n",
    "        B = pd.concat([B,conn_MBoutput[i]], axis=0, join='outer', ignore_index=True,sort=False)        \n",
    "conn_MBoutput = B\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuprint import fetch_synapses, NeuronCriteria as NC, SynapseCriteria as SC\n",
    "\n",
    "neuron_criteria = NC(status='Traced', type='MBON31', cropped=False, inputRois=['gL(R)'], min_roi_inputs=100, min_pre=400)\n",
    "LAL_tbar_criteria = SC(rois='LAL(R)', type='pre', primary_only=True)\n",
    "LAL_tbars = fetch_synapses(neuron_criteria, LAL_tbar_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the synapse positions in a 2D projection\n",
    "p = figure()\n",
    "p.scatter(LAL_tbars['x'], LAL_tbars['y'])\n",
    "p.y_range.flipped = True\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fetch MBoutput that have connection to LAL\n",
    "from neuprint import fetch_synapse_connections, SynapseCriteria as SC\n",
    "syn_LAL = []\n",
    "K = len(type_MBoutput)\n",
    "for i in range(K):    \n",
    "    neuron_criteria = NC(status='Traced', type=type_MBoutput[i], cropped=False, min_pre=3)\n",
    "    LAL_syn_criteria = SC(rois=['LAL(R)','LAL(L)'], primary_only=True)\n",
    "    try:\n",
    "        syn_df = fetch_synapse_connections(neuron_criteria, None, LAL_syn_criteria)\n",
    "        print(str(i)+'<->LAL:found connection(s)')\n",
    "        if len(syn_LAL)>0: \n",
    "                syn_LAL = pd.concat([syn_LAL,syn_df], axis=0, join='outer', ignore_index=True,sort=False)        \n",
    "        else:\n",
    "                syn_LAL = syn_df\n",
    "    except:\n",
    "            print(str(i)+'<->LAL:no connection')\n",
    "    \n",
    "syn_LAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retieve the types of pre-synaptic neurons\n",
    "pre_neurons, _ = fetch_neurons(syn_LAL['bodyId_pre'].unique())\n",
    "syn_LAL = merge_neuron_properties(pre_neurons, syn_LAL, 'type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_LAL['type_pre'].value_counts()\n",
    "#syn_LAL['type_post'].value_counts()\n",
    "type_presyn_LAL = syn_LAL['type_pre'].unique()\n",
    "type_postsyn_LAL = syn_LAL['type_post'].unique()\n",
    "type_presyn_LAL = list(set(type_presyn_LAL)&set(type_MBoutput))\n",
    "type_postsyn_LAL = list(set(type_postsyn_LAL)&set(type_MBoutput))\n",
    "\n",
    "len(type_presyn_LAL)\n",
    "\n",
    "# Retrieve the types of pre-synaptic neurons\n",
    "top10_counts = syn_LAL['type_pre'].value_counts().head(10)\n",
    "top10_counts\n",
    "colormap = dict(zip(top10_counts.index, bokeh.palettes.Category10[10]))\n",
    "points = syn_LAL.query('type_pre in @top10_counts.index').copy()\n",
    "points['color'] = points['type_pre'].map(colormap)\n",
    "\n",
    "p = figure()\n",
    "scatter = p.scatter(points['x_post'], points['z_post'], color=points['color'])\n",
    "p.y_range.flipped = True\n",
    "show(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the types of the post-synaptic neurons\n",
    "pre_neurons, _ = fetch_neurons(syn_LAL['bodyId_pre'].unique())\n",
    "LAL_conns = merge_neuron_properties(post_neurons, syn_LAL, 'type')\n",
    "\n",
    "top5_counts = LAL_conns['type_post'].value_counts().head(5)\n",
    "top5_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = dict(zip(top5_counts.index, bokeh.palettes.Category10[5]))\n",
    "points = LAL_conns.query('type_post in @top5_counts.index').copy()\n",
    "points['color'] = points['type_post'].map(colormap)\n",
    "\n",
    "p = figure()\n",
    "p.scatter(points['x_post'], points['z_post'], color=points['color'])\n",
    "p.y_range.flipped = True\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download some skeletons as DataFrames and attach columns for bodyId and color\n",
    "skeletons = []\n",
    "for i , bodyId in enumerate(syn_LAL['bodyId_pre'].unique()):\n",
    "    s = c.fetch_skeleton(bodyId, format='pandas')\n",
    "    s['bodyId'] = bodyId\n",
    "    s['color'] = bokeh.palettes.Category10[10][1]\n",
    "    skeletons.append(s)\n",
    "\n",
    "# Combine into one big table for convenient processing\n",
    "skeletons = pd.concat(skeletons, ignore_index=True)\n",
    "skeletons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join parent/child nodes for plotting as line segments below.\n",
    "segments = skeletons.merge(skeletons, 'inner',\n",
    "                           left_on=['bodyId', 'rowId'],\n",
    "                           right_on=['bodyId', 'link'],\n",
    "                           suffixes=['_child', '_parent'])\n",
    "\n",
    "p = figure()\n",
    "p.y_range.flipped = True\n",
    "\n",
    "# Plot skeleton segments (in 2D)\n",
    "p.segment(x0='x_child', x1='x_parent',\n",
    "          y0='z_child', y1='z_parent',\n",
    "          color='color_child',\n",
    "          source=segments)\n",
    "\n",
    "# Also plot the synapses from the above example\n",
    "p.scatter(points['x_post'], points['z_post'], color=points['color'])\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for downwind neuron\n",
    "bodyId_DnWiN = [547240545,452162916,392411222,357945155,484554412,455815977,546287086,488572496,547310321,516214576,516965856,486876782]\n",
    "neuron_DnWiN, _ = fetch_neurons(bodyId_DnWiN)\n",
    "type_DnWiN = neuron_DnWiN['type'].unique()\n",
    "type_DnWiN\n",
    "bodyId_df = bodyId_DnWiN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_target = neuron_DnWiN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#presynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(None,neuron_target)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "col_sum = matrix_df.sum(1)\n",
    "sort_I = np.argsort(col_sum)\n",
    "matrix_df = matrix_df.iloc[sort_I,:]\n",
    "matrix_df = matrix_df.loc[col_sum>50,:]\n",
    "matrix_df.hvplot.heatmap(height=600, width=400).opts(xrotation=60,title='presynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postsynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(neuron_target,None)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "row_sum = matrix_df.sum(0)\n",
    "sort_I = np.argsort(row_sum)\n",
    "matrix_df = matrix_df.iloc[:,np.flipud(sort_I)]\n",
    "matrix_df = matrix_df.loc[:,row_sum>50]\n",
    "matrix_df.hvplot.heatmap(height=400, width=600).opts(xrotation=60,title='postsynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for downwind neuron\n",
    "bodyId_DnWiN2 = [739572008,894551935,613083317,644148539,644153089,5813054659]\n",
    "neuron_DnWiN2, _ = fetch_neurons(bodyId_DnWiN2)\n",
    "type_DnWiN2 = neuron_DnWiN2['type'].unique()\n",
    "print(type_DnWiN2)\n",
    "bodyId_df = bodyId_DnWiN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_target = neuron_DnWiN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#presynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(None,neuron_target)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "col_sum = matrix_df.sum(1)\n",
    "sort_I = np.argsort(col_sum)\n",
    "matrix_df = matrix_df.iloc[sort_I,:]\n",
    "matrix_df = matrix_df.loc[col_sum>50,:]\n",
    "matrix_df.hvplot.heatmap(height=600, width=400).opts(xrotation=60,title='presynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postsynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(neuron_target,None)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "row_sum = matrix_df.sum(0)\n",
    "sort_I = np.argsort(row_sum)\n",
    "matrix_df = matrix_df.iloc[:,np.flipud(sort_I)]\n",
    "matrix_df = matrix_df.loc[:,row_sum>50]\n",
    "matrix_df.hvplot.heatmap(height=400, width=600).opts(xrotation=60,title='postsynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for FB\n",
    "type_FB_intrinsic = ['Delta6D','Delta6E','Delta6F_a','Delta6B']\n",
    "K = len(type_FB_intrinsic)\n",
    "neuron_FB_intrinsic = []\n",
    "\n",
    "for i in range(K):\n",
    "    criteria = NC(type=type_FB_intrinsic[i],regex=True)\n",
    "    neuron_df, _ = fetch_neurons(criteria)\n",
    "    if len(neuron_FB_intrinsic)>0:\n",
    "        neuron_FB_intrinsic = pd.concat([neuron_FB_intrinsic,neuron_df])\n",
    "    else:\n",
    "        neuron_FB_intrinsic = neuron_df\n",
    "\n",
    "bodyId_FB_intrinsic = neuron_FB_intrinsic['bodyId']\n",
    "neuron_FB_intrinsic.head\n",
    "neuron_target = neuron_FB_intrinsic\n",
    "#neuron_FB_intrinsic, _ = fetch_neurons(type=type_FB_intrinsic)\n",
    "# axis=0, join='outer', ignore_index=True,sort=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_target = neuron_FB_intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#presynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(None,neuron_target)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "col_sum = matrix_df.sum(1)\n",
    "sort_I = np.argsort(col_sum)\n",
    "matrix_df = matrix_df.iloc[sort_I,:]\n",
    "matrix_df = matrix_df.loc[col_sum>200,:]\n",
    "matrix_df.hvplot.heatmap(height=1000, width=400).opts(xrotation=60,title='presynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postsynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(neuron_target,None)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "row_sum = matrix_df.sum(0)\n",
    "sort_I = np.argsort(row_sum)\n",
    "matrix_df = matrix_df.iloc[:,np.flipud(sort_I)]\n",
    "matrix_df = matrix_df.loc[:,row_sum>200]\n",
    "matrix_df.hvplot.heatmap(height=400, width=1000).opts(xrotation=60,title='postsynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lvPN\n",
    "bodyId_lvPN = [1759999022,1943811511,5813069532,5813079228,1672293837,1670973150,1730656082,1670969304,329599710]\n",
    "neuron_lvPN, _ = fetch_neurons(bodyId_lvPN)\n",
    "type_lvPN = neuron_lvPN['type'].unique()\n",
    "print(type_lvPN)\n",
    "bodyId_df = bodyId_lvPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_target = neuron_lvPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#presynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(None,neuron_target)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('instance_pre', 'type_post'))\n",
    "col_sum = matrix_df.sum(1)\n",
    "sort_I = np.argsort(col_sum)\n",
    "matrix_df = matrix_df.iloc[sort_I,:]\n",
    "matrix_df = matrix_df.loc[col_sum>10,:]\n",
    "matrix_df.hvplot.heatmap(height=600, width=400).opts(xrotation=60,title='presynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postsynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(neuron_target,None)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "row_sum = matrix_df.sum(0)\n",
    "sort_I = np.argsort(row_sum)\n",
    "matrix_df = matrix_df.iloc[:,np.flipud(sort_I)]\n",
    "matrix_df = matrix_df.loc[:,row_sum>10]\n",
    "matrix_df.hvplot.heatmap(height=400, width=800).opts(xrotation=60,title='postsynaptic partners')\n",
    "matrix_df.iloc[:,1:25].hvplot.heatmap(height=400, width=800).opts(xrotation=60,title='postsynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for UpWiN_all\n",
    "bodyId_UpWiN = [481731074,326888609,296535619,266187559,5813010748,298591130,296199149,484134987,389307767,424443911,390003153,5813039910]\n",
    "neuron_UpWiN, _ = fetch_neurons(bodyId_UpWiN)\n",
    "type_UpWiN = neuron_UpWiN['type'].unique()\n",
    "print(type_UpWiN)\n",
    "bodyId_df = bodyId_UpWiN\n",
    "neuron_target = neuron_UpWiN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#presynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(None,neuron_target)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "col_sum = matrix_df.sum(1)\n",
    "sort_I = np.argsort(col_sum)\n",
    "matrix_df = matrix_df.iloc[sort_I,:]\n",
    "matrix_df = matrix_df.loc[col_sum>50,:]\n",
    "matrix_df.hvplot.heatmap(height=600, width=400).opts(xrotation=60,title='presynaptic partners')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postsynaptic partner\n",
    "neuron_df, conn_df = fetch_adjacencies(neuron_target,None)\n",
    "conn_df = merge_neuron_properties(neuron_df, conn_df, ['type', 'instance'])\n",
    "conn_df.sort_values('weight', ascending=False)\n",
    "matrix_df = connection_table_to_matrix(conn_df, ('type_pre', 'type_post'))\n",
    "row_sum = matrix_df.sum(0)\n",
    "sort_I = np.argsort(row_sum)\n",
    "matrix_df = matrix_df.iloc[:,np.flipud(sort_I)]\n",
    "matrix_df = matrix_df.loc[:,row_sum>10]\n",
    "matrix_df.hvplot.heatmap(height=400, width=800).opts(xrotation=60,title='postsynaptic partners')\n",
    "matrix_df.iloc[:,0:25].hvplot.heatmap(height=400, width=800).opts(xrotation=60,title='postsynaptic partners')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plastix",
   "language": "python",
   "name": "plastix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
